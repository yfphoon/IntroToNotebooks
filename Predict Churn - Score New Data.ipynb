{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Customer Churn Model Scoring\n#### The objectives of this lab is:\n- score **new** customer data against a pre-built model\n- schedule the notebook to run via the Notebook scheduler\n\n### Step 1: Download new customer data"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "import wget\nurl_customer='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/new_customer_churn_data.csv'\n\n#remove existing files before downloading\n!rm -f new_customer_churn_data.csv\n\ncustomerFilename=wget.download(url_customer)\n\n!ls -l new_customer_churn_data.csv", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 2: Read data into a Spark DataFrame\n**Note**: the new dataset does not contain the label column"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "newData= sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(customerFilename)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "newData = newData.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\nnewData.toPandas().head()", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 3: Load Saved Model"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from pyspark.ml import PipelineModel\nmodel1_loaded = PipelineModel.load(\"PredictChurn.churnModel\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 4: Score the new data\nNote: The scored output contains the predicted values and confidence scores"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "result = model1_loaded.transform(newData)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 5: Export Score into a csv file"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "#Select ID, prediction and probability fields from the result dataframe\n\nr1=result.select(result[\"ID\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\nr1.show(5,False)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "#### Decompose the probability column\nThe probability column contains a vector for each record, and the elements must be extracted"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "from pyspark.sql import Row\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import udf\nfrom pyspark.ml.linalg import Vectors\n\nudf_0 = udf(lambda vector: float(vector[0]), DoubleType())\nudf_1 = udf(lambda vector: float(vector[1]), DoubleType())\n\nr2 = (r1.select(r1[\"ID\"], r1[\"prediction\"],r1[\"probability\"])\n    .withColumn('probability_0', udf_0(r1.probability))\n    .withColumn('probability_1', udf_1(r1.probability))\n    .drop(\"probability\"))\n\nr2.show(10, False)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "#### Connect to Object Storage\nIn order to write the scores to Object Storage, specify the credentials to connect to your instance of Object Storage.  The easiet way to do that is:\n- If you do not already have a file in Object Storage, load a file into it using the **Files** interface\n- Choose \"*Insert SparkSession DataFame*\" to generate the credentials and code to connect to Object Storage\n\n![Load Files](https://raw.githubusercontent.com/yfphoon/IntroToNotebooks/master/images/upload_files.png)\n\n- Edit the code to comment out or edit the code that reads the file.  The edited code cell should look like this\n\n![credentials](https://raw.githubusercontent.com/yfphoon/IntroToNotebooks/master/images/generated_credentials.png)\n\n"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# insert code here\n\n", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "#### Write sores .csv file"}, {"cell_type": "markdown", "metadata": {}, "source": "The code cell below specifies the options for saving the csv file.  Check that you have specified the **TARGET_CONTAINER** to point to your project."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from ingest.Connectors import Connectors\n\nobjectstoresaveOptions = {\n        Connectors.BluemixObjectStorage.AUTH_URL          : credentials['auth_url'],\n        Connectors.BluemixObjectStorage.USERID            : credentials['user_id'],\n        Connectors.BluemixObjectStorage.PASSWORD          : credentials['password'],\n        Connectors.BluemixObjectStorage.PROJECTID         : credentials['project_id'],\n        Connectors.BluemixObjectStorage.REGION            : credentials['region'],\n        Connectors.BluemixObjectStorage.TARGET_CONTAINER  : 'IntroToNotebooks',\n        Connectors.BluemixObjectStorage.TARGET_FILE_NAME  : 'churn_scores.csv',\n        Connectors.BluemixObjectStorage.TARGET_WRITE_MODE : 'write'}\n\n\nr2.write.format(\"com.ibm.spark.discover\").options(**objectstoresaveOptions).save()", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "r3 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .load(bmos.url('IntroToNotebooks', 'churn_scores.csv'))\nr3.select(r3[\"_c0\"].alias(\"ID\"), r3[\"_c1\"].alias(\"prediction\"), r3[\"_c2\"].alias(\"probability_0\"), r3[\"_c3\"].alias(\"probability_1\")).show(5, False)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "### Step 6: Schedule this notebook to run at a time and frequency of your choice\nClick on the \"clock\" icon at the top right"}, {"cell_type": "markdown", "metadata": {}, "source": "You have come to the end of this notebook"}, {"cell_type": "markdown", "metadata": {}, "source": "** Sidney Phoon** <br/>\nyfphoon@us.ibm.com<br/>\nAug, 2017"}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "version": "2.7.11", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}}, "nbformat": 4}